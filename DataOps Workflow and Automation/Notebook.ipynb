{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Git repository\n",
    "git init\n",
    "\n",
    "# Initialize DVC\n",
    "dvc init\n",
    "\n",
    "# Add and commit code files to Git\n",
    "git add data_preprocessing.py model_training.py\n",
    "git commit -m \"Add data preprocessing and model training scripts\"\n",
    "\n",
    "# Add large dataset to DVC\n",
    "dvc add data/large_dataset.csv\n",
    "git add data/large_dataset.csv.dvc\n",
    "git commit -m \"Add reference to large dataset\"\n",
    "\n",
    "# Create a new branch for feature development\n",
    "git checkout -b feature/new_data_transformation\n",
    "\n",
    "# Make changes to the code and data\n",
    "# ... (modify data_preprocessing.py)\n",
    "# ... (update large_dataset.csv)\n",
    "\n",
    "# Update DVC tracked file\n",
    "dvc add data/large_dataset.csv\n",
    "\n",
    "# Commit changes\n",
    "git add data_preprocessing.py data/large_dataset.csv.dvc\n",
    "git commit -m \"Implement new data transformation and update dataset\"\n",
    "\n",
    "# Push changes to remote repository\n",
    "git push origin feature/new_data_transformation\n",
    "dvc push\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature branch\n",
    "git checkout -b feature/enhanced_feature_engineering\n",
    "\n",
    "# Make changes to feature engineering code\n",
    "vim feature_engineering.py\n",
    "\n",
    "# Update dataset with new features\n",
    "python feature_engineering.py\n",
    "\n",
    "# Track updated dataset with DVC\n",
    "dvc add data/enhanced_features.csv\n",
    "\n",
    "# Commit changes\n",
    "git add feature_engineering.py data/enhanced_features.csv.dvc\n",
    "git commit -m \"Implement enhanced feature engineering\"\n",
    "\n",
    "# Push changes to remote repository\n",
    "git push origin feature/enhanced_feature_engineering\n",
    "dvc push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Modular components\n",
    "\n",
    "def extract_data(**kwargs):\n",
    "    # Extract data from source\n",
    "    data = {\"raw_data\": [1, 2, 3, 4, 5]}\n",
    "    return data\n",
    "\n",
    "def clean_data(**kwargs):\n",
    "    ti = kwargs['ti']\n",
    "    data = ti.xcom_pull(task_ids='extract_data')\n",
    "    # Clean the data\n",
    "    cleaned_data = [x for x in data['raw_data'] if x % 2 == 0]\n",
    "    return {\"cleaned_data\": cleaned_data}\n",
    "\n",
    "def transform_data(**kwargs):\n",
    "    ti = kwargs['ti']\n",
    "    data = ti.xcom_pull(task_ids='clean_data')\n",
    "    # Transform the data\n",
    "    transformed_data = [x * 2 for x in data['cleaned_data']]\n",
    "    return {\"transformed_data\": transformed_data}\n",
    "\n",
    "def load_data(**kwargs):\n",
    "    ti = kwargs['ti']\n",
    "    data = ti.xcom_pull(task_ids='transform_data')\n",
    "    # Load data (simplified example)\n",
    "    print(f\"Loading data: {data['transformed_data']}\")\n",
    "\n",
    "# DAG definition\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2023, 1, 1),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'modular_data_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='A modular data pipeline example',\n",
    "    schedule_interval=timedelta(days=1),\n",
    ")\n",
    "\n",
    "# Task definitions\n",
    "extract_task = PythonOperator(\n",
    "    task_id='extract_data',\n",
    "    python_callable=extract_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "clean_task = PythonOperator(\n",
    "    task_id='clean_data',\n",
    "    python_callable=clean_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "transform_task = PythonOperator(\n",
    "    task_id='transform_data',\n",
    "    python_callable=transform_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "load_task = PythonOperator(\n",
    "    task_id='load_data',\n",
    "    python_callable=load_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Define task dependencies\n",
    "extract_task >> clean_task >> transform_task >> load_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "import random\n",
    "import luigi\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataProcessingError(Exception):\n",
    "    pass\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def fetch_data(source):\n",
    "    logger.info(f\"Fetching data from {source}\")\n",
    "    if random.random() < 0.5:  # Simulate intermittent failure\n",
    "        raise ConnectionError(\"Failed to connect to data source\")\n",
    "    return [1, 2, 3, 4, 5]\n",
    "\n",
    "def process_data(data):\n",
    "    logger.info(\"Processing data\")\n",
    "    if not data:\n",
    "        raise DataProcessingError(\"Empty dataset\")\n",
    "    return [x * 2 for x in data]\n",
    "\n",
    "def save_data(data):\n",
    "    logger.info(f\"Saving data: {data}\")\n",
    "    # Simulate data saving\n",
    "\n",
    "def run_pipeline(data_source):\n",
    "    try:\n",
    "        raw_data = fetch_data(data_source)\n",
    "        processed_data = process_data(raw_data)\n",
    "        save_data(processed_data)\n",
    "        logger.info(\"Pipeline completed successfully\")\n",
    "    except ConnectionError as e:\n",
    "        logger.error(f\"Failed to fetch data after multiple attempts: {e}\")\n",
    "    except DataProcessingError as e:\n",
    "        logger.error(f\"Data processing error: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error in pipeline: {e}\")\n",
    "    finally:\n",
    "        logger.info(\"Cleaning up resources\")\n",
    "        # Perform any necessary cleanup\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline(\"example_source\")\n",
    "\n",
    "# Airflow DAG definition\n",
    "default_args = {\n",
    "    'owner': 'dataops_team',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2023, 1, 1),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'simple_etl_pipeline',\n",
    "    default_args=default_args,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "import random\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataProcessingError(Exception):\n",
    "    pass\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def fetch_data(source):\n",
    "    logger.info(f\"Fetching data from {source}\")\n",
    "    if random.random() < 0.5:  # Simulate intermittent failure\n",
    "        raise ConnectionError(\"Failed to connect to data source\")\n",
    "    return [1, 2, 3, 4, 5]\n",
    "\n",
    "def process_data(data):\n",
    "    logger.info(\"Processing data\")\n",
    "    if not data:\n",
    "        raise DataProcessingError(\"Empty dataset\")\n",
    "    return [x * 2 for x in data]\n",
    "\n",
    "def save_data(data):\n",
    "    logger.info(f\"Saving data: {data}\")\n",
    "    # Simulate data saving\n",
    "\n",
    "def run_pipeline(data_source):\n",
    "    try:\n",
    "        raw_data = fetch_data(data_source)\n",
    "        processed_data = process_data(raw_data)\n",
    "        save_data(processed_data)\n",
    "        logger.info(\"Pipeline completed successfully\")\n",
    "    except ConnectionError as e:\n",
    "        logger.error(f\"Failed to fetch data after multiple attempts: {e}\")\n",
    "    except DataProcessingError as e:\n",
    "        logger.error(f\"Data processing error: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error in pipeline: {e}\")\n",
    "    finally:\n",
    "        logger.info(\"Cleaning up resources\")\n",
    "        # Perform any necessary cleanup\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline(\"example_source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'dataops_team',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2023, 1, 1),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'simple_etl_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='A simple ETL pipeline using Apache Airflow',\n",
    "    schedule_interval=timedelta(days=1),\n",
    ")\n",
    "\n",
    "def extract_data(**kwargs):\n",
    "    # Simulating data extraction\n",
    "    data = [1, 2, 3, 4, 5]\n",
    "    kwargs['ti'].xcom_push(key='raw_data', value=data)\n",
    "\n",
    "def transform_data(**kwargs):\n",
    "    ti = kwargs['ti']\n",
    "    data = ti.xcom_pull(key='raw_data', task_ids='extract_task')\n",
    "    transformed_data = [x * 2 for x in data]\n",
    "    ti.xcom_push(key='transformed_data', value=transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(**kwargs):\n",
    "    ti = kwargs['ti']\n",
    "    data = ti.xcom_pull(key='transformed_data', task_ids='transform_task')\n",
    "    print(f\"Loading data: {data}\")\n",
    "\n",
    "extract_task = PythonOperator(\n",
    "    task_id='extract_task',\n",
    "    python_callable=extract_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "transform_task = PythonOperator(\n",
    "    task_id='transform_task',\n",
    "    python_callable=transform_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "load_task = PythonOperator(\n",
    "    task_id='load_task',\n",
    "    python_callable=load_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "cleanup_task = BashOperator(\n",
    "    task_id='cleanup_task',\n",
    "    bash_command='echo \"Performing cleanup operations\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "extract_task >> transform_task >> load_task >> cleanup_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import luigi\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "\n",
    "class DownloadData(luigi.Task):\n",
    "    date = luigi.DateParameter()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(f\"data/raw_{self.date}.csv\")\n",
    "\n",
    "    def run(self):\n",
    "        # Simulate downloading data\n",
    "        df = pd.DataFrame({'feature': range(100), 'target': [0, 1] * 50})\n",
    "        df.to_csv(self.output().path, index=False)\n",
    "\n",
    "class PrepareData(luigi.Task):\n",
    "    date = luigi.DateParameter()\n",
    "\n",
    "    def requires(self):\n",
    "        return DownloadData(date=self.date)\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(f\"data/prepared_{self.date}.csv\")\n",
    "\n",
    "    def run(self):\n",
    "        df = pd.read_csv(self.input().path)\n",
    "        # Perform data preparation\n",
    "        df['feature_squared'] = df['feature'] ** 2\n",
    "        df.to_csv(self.output().path, index=False)\n",
    "\n",
    "class TrainModel(luigi.Task):\n",
    "    date = luigi.DateParameter()\n",
    "\n",
    "    def requires(self):\n",
    "        return PrepareData(date=self.date)\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(f\"models/model_{self.date}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(self):\n",
    "        df = pd.read_csv(self.input().path)\n",
    "        X = df[['feature', 'feature_squared']]\n",
    "        y = df['target']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        model = RandomForestClassifier()\n",
    "        model.fit(X_train, y_train)\n",
    "        with self.output().open('wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "class EvaluateModel(luigi.Task):\n",
    "    date = luigi.DateParameter()\n",
    "\n",
    "    def requires(self):\n",
    "        return {\n",
    "            'data': PrepareData(date=self.date),\n",
    "            'model': TrainModel(date=self.date)\n",
    "        }\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(f\"reports/evaluation_{self.date}.txt\")\n",
    "\n",
    "    def run(self):\n",
    "        with self.input()['model'].open('rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        df = pd.read_csv(self.input()['data'].path)\n",
    "        X = df[['feature', 'feature_squared']]\n",
    "        y = df['target']\n",
    "        accuracy = model.score(X, y)\n",
    "        with self.output().open('w') as f:\n",
    "            f.write(f\"Model accuracy: {accuracy}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    luigi.build([EvaluateModel(date=luigi.DateParameter().parse('2023-05-01'))], local_scheduler=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
