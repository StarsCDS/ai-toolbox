{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAuha6n-X511"
      },
      "source": [
        "\n",
        "# **Working with semi-structured data in JSON format**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI8i4bijXX_4",
        "outputId": "effb1121-c60d-4522-8adc-ad4632387f58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  product_id        name    price specs.screen specs.battery specs.camera  \\\n",
            "0       P001  Smartphone   599.99     6.2 inch      4000 mAh        12 MP   \n",
            "1       P002      Laptop  1299.99          NaN           NaN          NaN   \n",
            "\n",
            "  specs.processor specs.ram specs.storage  \n",
            "0             NaN       NaN           NaN  \n",
            "1        Intel i7     16 GB    512 GB SSD  \n",
            "Processor of the laptop:\n",
            "Intel i7\n"
          ]
        }
      ],
      "source": [
        "#Import the necessary libraries\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Example of semi-structured data in JSON format\n",
        "json_data = '''\n",
        "[\n",
        "  {\n",
        "    \"product_id\": \"P001\",\n",
        "    \"name\": \"Smartphone\",\n",
        "    \"price\": 599.99,\n",
        "    \"specs\": {\n",
        "      \"screen\": \"6.2 inch\",\n",
        "      \"battery\": \"4000 mAh\",\n",
        "      \"camera\": \"12 MP\"\n",
        "    }\n",
        "  },\n",
        "  {\n",
        "    \"product_id\": \"P002\",\n",
        "    \"name\": \"Laptop\",\n",
        "    \"price\": 1299.99,\n",
        "    \"specs\": {\n",
        "      \"processor\": \"Intel i7\",\n",
        "      \"ram\": \"16 GB\",\n",
        "      \"storage\": \"512 GB SSD\"\n",
        "    }\n",
        "  }\n",
        "]\n",
        "'''\n",
        "\n",
        "# Parse JSON data\n",
        "data = json.loads(json_data)\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df = pd.json_normalize(data)\n",
        "\n",
        "print(df)\n",
        "\n",
        "# Accessing nested data\n",
        "print(\"Processor of the laptop:\")\n",
        "print(df.loc[df['name'] == 'Laptop', 'specs.processor'].values[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEgMqlySYVbJ"
      },
      "source": [
        "# **Batch data ingestion process using Python**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vD3J-2WhYG-K"
      },
      "outputs": [],
      "source": [
        "#Install required libraries\n",
        "!pip install schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VQfS0FBRYHFl"
      },
      "outputs": [],
      "source": [
        "# Importing necessary library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sqlalchemy import create_engine\n",
        "import schedule\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "L_rpNmyNYGYz"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic data. This data generator should not be edited as it simulates a fixed schema dataset.\n",
        "def generate_synthetic_data():\n",
        "    np.random.seed(0)\n",
        "    dates = pd.date_range(start='2021-01-01', periods=100, freq='D')\n",
        "    modes = ['Cash', 'Card', 'Online', 'Wallet']\n",
        "    categories = ['Grocery', 'Electronics', 'Apparel', 'Dining']\n",
        "    subcategories = ['Vegetables', 'Mobiles', 'Clothing', 'Restaurants']\n",
        "    notes = ['Purchase', 'Payment', 'Refund', 'Expense']\n",
        "    amounts = np.random.uniform(100, 5000, size=100)\n",
        "    income_expense = ['Income', 'Expense']\n",
        "    currency = 'INR'\n",
        "\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'Date': dates,\n",
        "        'Mode': np.random.choice(modes, size=100),\n",
        "        'Category': np.random.choice(categories, size=100),\n",
        "        'Subcategory': np.random.choice(subcategories, size=100),\n",
        "        'Note': np.random.choice(notes, size=100),\n",
        "        'Amount': amounts,\n",
        "        'Income/Expense': np.random.choice(income_expense, size=100),\n",
        "        'Currency': currency\n",
        "    })\n",
        "\n",
        " \n",
        "    df.to_csv('daily_transactions.csv', index=False)\n",
        "\n",
        "# Call the function to generate data\n",
        "generate_synthetic_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLJePh0ZYgZ-",
        "outputId": "06fcfc47-b749-48ad-c36a-5825e068ec3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch ingestion stopped by user.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Function to simulate batch data ingestion from a CSV file into an SQLite database.\n",
        "Note:\n",
        "- No need for database credentials or network setup.\n",
        "\"\"\"\n",
        "def batch_ingest():\n",
        "\n",
        "    df = pd.read_csv('daily_transactions.csv')\n",
        "    df['transaction_date'] = pd.to_datetime(df['Date'])\n",
        "    df['amount'] = df['Amount'].astype(float)\n",
        "\n",
        "    # Create an SQLite engine\n",
        "    engine = create_engine('sqlite:///mydatabase.db')\n",
        "\n",
        "    # Ingest data into the SQLite database\n",
        "    df.to_sql('transactions', engine, if_exists='append', index=False)\n",
        "\n",
        "    print(f\"Ingested {len(df)} records at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Setting up a scheduler to run the batch ingestion daily at 1:00 AM\n",
        "schedule.every().day.at(\"01:00\").do(batch_ingest)\n",
        "\n",
        "# Infinite loop to keep the script running\n",
        "# Note: This will run indefinitely. User must manually stop execution.\n",
        "try:\n",
        "    while True:\n",
        "        schedule.run_pending()\n",
        "        time.sleep(5)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Batch ingestion stopped by user.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7LjIFVCZAjs"
      },
      "source": [
        "# **Real-time data streaming using Python and Kafka**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwPuUC-CZBTF"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install kafka-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "g4PoBEwNZFru"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "from kafka import KafkaConsumer\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEMtu4fgZLQ9"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Creates and returns a Kafka consumer listening on a specified topic with\n",
        "auto-commit enabled and latest offset reset.\n",
        "\"\"\"\n",
        "def create_kafka_consumer():\n",
        "    consumer = KafkaConsumer(\n",
        "        'user_activity',\n",
        "        bootstrap_servers=['localhost:9092'],\n",
        "        auto_offset_reset='latest',\n",
        "        enable_auto_commit=True,\n",
        "        value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
        "    )\n",
        "    return consumer\n",
        "\"\"\"\n",
        "Processes incoming Kafka messages. It performs operations based on the type of user activity,\n",
        "such as updating profiles or triggering security alerts.\n",
        "\"\"\"\n",
        "def process_messages(consumer):\n",
        "    for message in consumer:\n",
        "        user_activity = message.value\n",
        "        print(f\"Received user activity: {user_activity}\")\n",
        "        # Example: Update user profile or trigger alert based on activity type.\n",
        "        if user_activity['activity_type'] == 'purchase':\n",
        "            # This is a placeholder for your update function\n",
        "            update_user_profile(user_activity['user_id'], user_activity['item_id'])\n",
        "        elif user_activity['activity_type'] == 'login' and user_activity['location'] != 'usual_location':\n",
        "            # This is a placeholder for your security alert function\n",
        "            trigger_security_alert(user_activity['user_id'], user_activity['location'])\n",
        "\n",
        "\n",
        "def update_user_profile(user_id, item_id):\n",
        "    \"\"\"Placeholder function to update user profile\"\"\"\n",
        "    print(f\"Updating profile for user {user_id} with item {item_id}\")\n",
        "\n",
        "def trigger_security_alert(user_id, location):\n",
        "    \"\"\"Placeholder function to trigger security alert\"\"\"\n",
        "    print(f\"Security alert for user {user_id} logging in from {location}\")\n",
        "\n",
        "# Create the Kafka consumer\n",
        "consumer = create_kafka_consumer()\n",
        "\n",
        "# Process incoming messages in real-time\n",
        "process_messages(consumer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-rMKMA6ZaFF"
      },
      "source": [
        "# **API-based data collection using Python and the requests library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TODUw9Q1aMWk"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "x-m8kaygaRnX"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9ANffk1aRtd",
        "outputId": "87b01c54-504d-4f63-ece9-4779c7e907ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              datetime  temperature  humidity    description\n",
            "0  2024-08-02 12:00:00        28.21        78  moderate rain\n",
            "1  2024-08-02 15:00:00        26.73        86  moderate rain\n",
            "2  2024-08-02 18:00:00        25.36        93     light rain\n",
            "3  2024-08-02 21:00:00        25.01        94     light rain\n",
            "4  2024-08-03 00:00:00        27.88        83     light rain\n"
          ]
        }
      ],
      "source": [
        "# Collect weather forecast data using the OpenWeatherMap API.\n",
        "def collect_weather_data(api_key, city, days=7):\n",
        "\n",
        "    base_url = \"http://api.openweathermap.org/data/2.5/forecast\"\n",
        "    params = {\n",
        "        'q': city,\n",
        "        'appid': api_key,\n",
        "        'units': 'metric'\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        weather_data = []\n",
        "        for item in data['list']:\n",
        "            weather_data.append({\n",
        "                'datetime': item['dt_txt'],\n",
        "                'temperature': item['main']['temp'],\n",
        "                'humidity': item['main']['humidity'],\n",
        "                'description': item['weather'][0]['description']\n",
        "            })\n",
        "        return weather_data\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        return None\n",
        "# Convert the list of weather data to a DataFrame and save it as a CSV file.\n",
        "def process_weather_data(weather_data, city):\n",
        "    if weather_data:\n",
        "        weather_df = pd.DataFrame(weather_data)\n",
        "        print(weather_df.head())\n",
        "        weather_df.to_csv(f\"{city}_weather_forecast.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No data to process.\")\n",
        "\n",
        "# Example usage of the function\n",
        "if __name__ == \"__main__\":\n",
        "    api_key = '3102dd06454b055d78f482f602e4c4a9'  # Example API key\n",
        "    city = 'Mexico'  # Example city\n",
        "    weather_data = collect_weather_data(api_key, city)\n",
        "\n",
        "    # Process the collected weather data\n",
        "    process_weather_data(weather_data, city)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKs_r_GdaJpk"
      },
      "source": [
        "# **Web scraping using Python and BeautifulSoup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uyUWiGDZb6V"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install requests beautifulsoup4 pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oKIUTtk4ZgbQ"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKkrLogXZgiv",
        "outputId": "5c4ed1c1-be57-4c80-af95-2ec568e5668f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               title    price availability\n",
            "0  The 10% Entrepreneur: Live Your Startup Dream ...  Â£27.55     In stock\n",
            "1  The Third Wave: An Entrepreneurâs Vision of ...  Â£12.61     In stock\n",
            "2                             If I Run (If I Run #1)  Â£49.97     In stock\n",
            "3         Counted With the Stars (Out from Egypt #1)  Â£17.97     In stock\n",
            "4               Like Never Before (Walker Family #2)  Â£28.77     In stock\n"
          ]
        }
      ],
      "source": [
        "# Scrape book information from a website using BeautifulSoup.\n",
        "def scrape_book_data(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    books = []\n",
        "    for book in soup.find_all('article', class_='product_pod'):\n",
        "        title = book.h3.a['title']\n",
        "        price = book.find('p', class_='price_color').text\n",
        "        availability = book.find('p', class_='instock availability').text.strip()\n",
        "\n",
        "        books.append({\n",
        "            'title': title,\n",
        "            'price': price,\n",
        "            'availability': availability\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(books)\n",
        "# Process the scraped book data and store the results in a CSV file.\n",
        "def process_book_data(books_df):\n",
        "    books_df.to_csv('data_science_books.csv', index=False)\n",
        "\n",
        "# Usage\n",
        "url = 'http://books.toscrape.com/catalogue/shoe-dog-a-memoir-by-the-creator-of-nike_831/index.html'\n",
        "books_df = scrape_book_data(url)\n",
        "\n",
        "print(books_df.head())\n",
        "process_book_data(books_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
