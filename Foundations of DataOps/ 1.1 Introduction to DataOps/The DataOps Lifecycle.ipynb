{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjgUhj5p5x6N"
      },
      "source": [
        "# DataOps pipeline for ML model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Synthetic data has been added to the provided code in the textbook . This generates synthetic customer data for a churn prediction scenario. It creates a dataset with 1000 samples, including customer tenure, two random features, and a binary churn indicator. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# synthetic customer data\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic data\n",
        "n_samples = 1000\n",
        "tenure_months = np.random.randint(1, 60, size=n_samples)\n",
        "feature1 = np.random.randn(n_samples)  # Example feature 1\n",
        "feature2 = np.random.randn(n_samples)  # Example feature 2\n",
        "churn = np.random.randint(0, 2, size=n_samples)  # Binary target variable\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'tenure_months': tenure_months,\n",
        "    'feature1': feature1,\n",
        "    'feature2': feature2,\n",
        "    'churn': churn\n",
        "})\n",
        "\n",
        "# Save the synthetic data to a CSV file\n",
        "data.to_csv('customer_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ni6oeweqSkJZ",
        "outputId": "02182933-2555-4c4a-d941-552f166c61a2"
      },
      "outputs": [],
      "source": [
        "# Code from the book \n",
        "# Example: Simple DataOps pipeline for ML model training\n",
        "\n",
        "# Data ingestion\n",
        "data = pd.read_csv('customer_data.csv')\n",
        "\n",
        "# Data preprocessing\n",
        "X = data.drop('churn', axis=1)\n",
        "y = data['churn']\n",
        "\n",
        "# Data quality check\n",
        "assert X.isnull().sum().sum() == 0, \"Missing values detected\"\n",
        "\n",
        "# Feature engineering\n",
        "X['tenure_years'] = X['tenure_months'] / 12\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model training\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Model evaluation\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Monitoring\n",
        "if accuracy < 0.8:\n",
        "    print(\"Warning: Model performance below threshold\")\n",
        "\n",
        "# Version control and logging (placeholder)\n",
        "print(\"Logging model version and performance metrics...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR1M-b_F6M7N"
      },
      "source": [
        "# CI/CD pipeline with Git and DVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the textbook, a basic CI/CD workflow using Git and DVC has been implemented, showcasing how code, data, and model versions can be managed and deployed in a DataOps context. The implementation sets up a Git repository, initializes DVC, creates placeholder dataset and model files, and tracks them with Git and DVC. It then performs simple steps for running tests and deploying."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install gitpython dvc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "from git import Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUp2I_DaZ7Za",
        "outputId": "413b9d2e-3af3-4307-f7df-896a2a9c48d8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define repository URL and directory\n",
        "repo_url = \"https://github.com/airex-lab/ai-toolbox.git\"\n",
        "repo_dir = \"/content/ai-toolbox\"\n",
        "\n",
        "# Function to initialize Git and DVC\n",
        "def initialize_repo():\n",
        "    if not os.path.exists(repo_dir):\n",
        "        print(f\"Cloning repository from {repo_url}...\")\n",
        "        Repo.clone_from(repo_url, repo_dir)\n",
        "    os.chdir(repo_dir)\n",
        "    if not os.path.exists(\".git\"):\n",
        "        subprocess.run([\"git\", \"init\"], check=True)\n",
        "    if not os.path.exists(\".dvc\"):\n",
        "        subprocess.run([\"dvc\", \"init\"], check=True)\n",
        "\n",
        "# Initialize repository\n",
        "initialize_repo()\n",
        "\n",
        "# Configure Git\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", \"Colab User\"], check=True)\n",
        "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", \"colab@example.com\"], check=True)\n",
        "\n",
        "# Create a placeholder dataset\n",
        "dataset_path = \"data/transactions.csv\"\n",
        "os.makedirs(os.path.dirname(dataset_path), exist_ok=True)\n",
        "with open(dataset_path, \"w\") as f:\n",
        "    f.write(\"TransactionID,Amount,Date\\n1,100,2024-01-01\\n2,150,2024-01-02\\n\")\n",
        "\n",
        "# Track the dataset with DVC and Git\n",
        "subprocess.run([\"dvc\", \"add\", dataset_path], check=True)\n",
        "subprocess.run([\"git\", \"add\", f\"{dataset_path}.dvc\"], check=True)\n",
        "\n",
        "# Check if there are changes to commit\n",
        "status = subprocess.run([\"git\", \"status\", \"--porcelain\"], capture_output=True, text=True)\n",
        "if status.stdout:\n",
        "    try:\n",
        "        subprocess.run([\"git\", \"commit\", \"-m\", \"Update transaction dataset\"], check=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Git commit failed: {e}\")\n",
        "else:\n",
        "    print(\"No changes to commit for dataset.\")\n",
        "\n",
        "# Create a placeholder model\n",
        "model_path = \"models/fraud_detection_model.pkl\"\n",
        "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
        "with open(model_path, \"wb\") as f:\n",
        "    f.write(b\"fake_model_data\")\n",
        "\n",
        "# Track the model with DVC and Git\n",
        "subprocess.run([\"dvc\", \"add\", model_path], check=True)\n",
        "subprocess.run([\"git\", \"add\", f\"{model_path}.dvc\"], check=True)\n",
        "\n",
        "# Check if there are changes to commit\n",
        "status = subprocess.run([\"git\", \"status\", \"--porcelain\"], capture_output=True, text=True)\n",
        "if status.stdout:\n",
        "    try:\n",
        "        subprocess.run([\"git\", \"commit\", \"-m\", \"Retrain fraud detection model\"], check=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Git commit failed: {e}\")\n",
        "else:\n",
        "    print(\"No changes to commit for model.\")\n",
        "\n",
        "# Simulate running tests\n",
        "def run_tests():\n",
        "    # Simulate a test result\n",
        "    return True\n",
        "\n",
        "if run_tests():\n",
        "    try:\n",
        "        subprocess.run([\"git\", \"push\", \"origin\", \"main\"], check=True)\n",
        "        subprocess.run([\"dvc\", \"push\"], check=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Git push or DVC push failed: {e}\")\n",
        "\n",
        "    # Simulate deployment\n",
        "    def deploy_model():\n",
        "        # Simulate deployment steps\n",
        "        print(\"Model deployed.\")\n",
        "\n",
        "    deploy_model()\n",
        "\n",
        "    # In a production environment\n",
        "    try:\n",
        "        subprocess.run([\"git\", \"pull\", \"origin\", \"main\"], check=True)\n",
        "        subprocess.run([\"dvc\", \"pull\"], check=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Git pull or DVC pull failed: {e}\")\n",
        "\n",
        "    # Simulate loading and using the model\n",
        "    def load_model():\n",
        "        # Simulate loading a model\n",
        "        print(\"Model loaded.\")\n",
        "\n",
        "    load_model()\n",
        "else:\n",
        "    print(\"Tests failed. Deployment aborted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8v3ZXA66jwG"
      },
      "source": [
        "# Agile DataOps sprints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each function is a placeholder for a specific task and hence created , added along with the code mentioned in the textbook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM_x7nwtStp8",
        "outputId": "87b0cb61-37e6-4737-d347-e11ef709a433"
      },
      "outputs": [],
      "source": [
        "# Conceptual example of agile DataOps sprints\n",
        "# Placeholder function implementations\n",
        "\n",
        "def update_data():\n",
        "    print(\"Updating data...\")\n",
        "\n",
        "def train_basic_model():\n",
        "    print(\"Training basic model...\")\n",
        "\n",
        "def deploy_to_test_environment():\n",
        "    print(\"Deploying to test environment...\")\n",
        "\n",
        "def collect_feedback():\n",
        "    print(\"Collecting feedback...\")\n",
        "\n",
        "def engineer_demographic_features():\n",
        "    print(\"Engineering demographic features...\")\n",
        "\n",
        "def retrain_model_with_demographics():\n",
        "    print(\"Retraining model with demographics...\")\n",
        "\n",
        "def setup_ab_testing_infrastructure():\n",
        "    print(\"Setting up A/B testing infrastructure...\")\n",
        "\n",
        "def deploy_model_variants():\n",
        "    print(\"Deploying model variants...\")\n",
        "\n",
        "def analyze_ab_test_results():\n",
        "    print(\"Analyzing A/B test results...\")\n",
        "\n",
        "def select_best_performing_model():\n",
        "    print(\"Selecting best performing model...\")\n",
        "\n",
        "def engineer_content_based_features():\n",
        "    print(\"Engineering content-based features...\")\n",
        "\n",
        "def train_hybrid_model():\n",
        "    print(\"Training hybrid model...\")\n",
        "\n",
        "def deploy_to_production():\n",
        "    print(\"Deploying to production...\")\n",
        "\n",
        "def monitor_performance():\n",
        "    print(\"Monitoring performance...\")\n",
        "\n",
        "def review_sprint_outcomes():\n",
        "    print(\"Reviewing sprint outcomes...\")\n",
        "\n",
        "def plan_next_sprint():\n",
        "    print(\"Planning next sprint...\")\n",
        "\n",
        "# code from the textbook \n",
        "\n",
        "def sprint_1():\n",
        "    # Goal: Implement basic collaborative filtering model\n",
        "    update_data()\n",
        "    train_basic_model()\n",
        "    deploy_to_test_environment()\n",
        "    collect_feedback()\n",
        "\n",
        "def sprint_2():\n",
        "    # Goal: Incorporate user demographics\n",
        "    update_data()\n",
        "    engineer_demographic_features()\n",
        "    retrain_model_with_demographics()\n",
        "    deploy_to_test_environment()\n",
        "    collect_feedback()\n",
        "\n",
        "def sprint_3():\n",
        "    # Goal: Implement A/B testing framework\n",
        "    setup_ab_testing_infrastructure()\n",
        "    deploy_model_variants()\n",
        "    analyze_ab_test_results()\n",
        "    select_best_performing_model()\n",
        "\n",
        "def sprint_4():\n",
        "    # Goal: Optimize model for cold start problem\n",
        "    engineer_content_based_features()\n",
        "    train_hybrid_model()\n",
        "    deploy_to_production()\n",
        "    monitor_performance()\n",
        "\n",
        "# Main agile loop\n",
        "for sprint in [sprint_1, sprint_2, sprint_3, sprint_4]:\n",
        "    sprint()\n",
        "    review_sprint_outcomes()\n",
        "    plan_next_sprint()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX2oIteU6sC-"
      },
      "source": [
        "# Role interactions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each function is a placeholder for a specific task and hence created , added along with the code mentioned in the textbook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p93ZqiKsStss",
        "outputId": "94cd0b4c-da4d-4c9f-e0cf-b5273e710dc6"
      },
      "outputs": [],
      "source": [
        "# Placeholder function implementations\n",
        "def extract_data_from_source():\n",
        "    print(\"Extracting data from source...\")\n",
        "    return \"raw_data\"\n",
        "\n",
        "def clean_and_validate(raw_data):\n",
        "    print(f\"Cleaning and validating {raw_data}...\")\n",
        "    return \"cleaned_data\"\n",
        "\n",
        "def store_in_data_warehouse(cleaned_data):\n",
        "    print(f\"Storing {cleaned_data} in data warehouse...\")\n",
        "\n",
        "def fetch_from_data_warehouse():\n",
        "    print(\"Fetching data from data warehouse...\")\n",
        "    return \"fetched_data\"\n",
        "\n",
        "def perform_analysis(data):\n",
        "    print(f\"Performing analysis on {data}...\")\n",
        "    return \"insights\"\n",
        "\n",
        "def create_visualizations(insights):\n",
        "    print(f\"Creating visualizations for {insights}...\")\n",
        "    return \"visualizations\"\n",
        "\n",
        "def present_to_stakeholders(visualizations):\n",
        "    print(f\"Presenting {visualizations} to stakeholders...\")\n",
        "\n",
        "def implement_ci_cd_pipeline():\n",
        "    print(\"Implementing CI/CD pipeline...\")\n",
        "\n",
        "def automate_data_quality_checks():\n",
        "    print(\"Automating data quality checks...\")\n",
        "\n",
        "def monitor_pipeline_performance():\n",
        "    print(\"Monitoring pipeline performance...\")\n",
        "\n",
        "def design_data_model():\n",
        "    print(\"Designing data model...\")\n",
        "\n",
        "def plan_data_integration_strategy():\n",
        "    print(\"Planning data integration strategy...\")\n",
        "\n",
        "def ensure_scalability_and_security():\n",
        "    print(\"Ensuring scalability and security...\")\n",
        "\n",
        "def define_business_objectives():\n",
        "    print(\"Defining business objectives...\")\n",
        "\n",
        "def review_data_insights():\n",
        "    print(\"Reviewing data insights...\")\n",
        "\n",
        "def make_data_driven_decisions():\n",
        "    print(\"Making data-driven decisions...\")\n",
        "\n",
        "# code from the textbook \n",
        "\n",
        "# Role-specific tasks\n",
        "def data_engineer_task():\n",
        "    # Set up data pipeline\n",
        "    raw_data = extract_data_from_source()\n",
        "    cleaned_data = clean_and_validate(raw_data)\n",
        "    store_in_data_warehouse(cleaned_data)\n",
        "\n",
        "def data_analyst_task():\n",
        "    # Analyze data and create visualizations\n",
        "    data = fetch_from_data_warehouse()\n",
        "    insights = perform_analysis(data)\n",
        "    visualizations = create_visualizations(insights)\n",
        "    present_to_stakeholders(visualizations)\n",
        "\n",
        "def dataops_engineer_task():\n",
        "    # Set up automated testing and deployment\n",
        "    implement_ci_cd_pipeline()\n",
        "    automate_data_quality_checks()\n",
        "    monitor_pipeline_performance()\n",
        "\n",
        "def data_architect_task():\n",
        "    # Design data system architecture\n",
        "    design_data_model()\n",
        "    plan_data_integration_strategy()\n",
        "    ensure_scalability_and_security()\n",
        "\n",
        "def business_stakeholder_task():\n",
        "    # Provide business context and evaluate results\n",
        "    define_business_objectives()\n",
        "    review_data_insights()\n",
        "    make_data_driven_decisions()\n",
        "\n",
        "# Main DataOps workflow\n",
        "def dataops_project():\n",
        "    data_architect_task()\n",
        "    data_engineer_task()\n",
        "    dataops_engineer_task()\n",
        "    data_analyst_task()\n",
        "    business_stakeholder_task()\n",
        "\n",
        "# Run the main DataOps project workflow\n",
        "dataops_project()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
