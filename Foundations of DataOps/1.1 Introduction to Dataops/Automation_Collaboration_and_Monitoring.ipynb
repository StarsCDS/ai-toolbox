{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated data pipeline with error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example builds an automated ETL (Extract, Transform, Load) pipeline using the Prefect library.\n",
    "\n",
    "1. First, it generates the synthetic data and saves it to the CSV file: raw_data.csv.\n",
    "2. Secondly it applies transformation that transforms the data by reversing strings in column raw_column, and saves the processed data into another CSV file: processed_data.csv.\n",
    "3. It includes error handling so that any failed task sends notifications.\n",
    "4. This pipeline creates a CSV with the processed data; in this CSV, each value under the original raw_column has been reversed under the new processed_column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install libraires\n",
    "!pip install pandas prefect\n",
    "!pip install -U prefect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Synthetic dataset\n",
    "import pandas as pd\n",
    "from prefect import task, flow\n",
    "\n",
    "# Define synthetic data creation task\n",
    "@task\n",
    "def create_synthetic_data():\n",
    "    data = {\n",
    "        'raw_column': ['apple', 'banana', 'cherry', 'date', 'elderberry']\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"raw_data.csv\", index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data extraction task\n",
    "@task\n",
    "def extract_data():\n",
    "    return pd.read_csv(\"raw_data.csv\")\n",
    "\n",
    "# Define data transformation task\n",
    "@task\n",
    "def transform_data(df):\n",
    "    df['processed_column'] = df['raw_column'].apply(lambda x: x[::-1])   # book code mentioned as some transformation its more generic\n",
    "    return df\n",
    "\n",
    "# Define data loading task\n",
    "@task\n",
    "def load_data(df):\n",
    "    df.to_csv(\"processed_data.csv\", index=False)\n",
    "\n",
    "# Define the flow\n",
    "@task\n",
    "def send_notification(message):\n",
    "    print(f\"ALERT: {message}\")\n",
    "    # Add your notification logic here\n",
    "\n",
    "def execute_task(task_func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Execute a task and handle any errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return task_func(*args, **kwargs)\n",
    "    except Exception as e:\n",
    "        send_notification(f\"{task_func.__name__} failed: {e}\")\n",
    "        raise  # Re-raise the exception to stop the flow\n",
    "\n",
    "# define the flow\n",
    "@flow(name=\"Automated ETL Pipeline\")\n",
    "def etl_pipeline():\n",
    "    try:\n",
    "        raw_data = execute_task(create_synthetic_data)\n",
    "        extracted_data = execute_task(extract_data)\n",
    "        processed_data = execute_task(transform_data, extracted_data)\n",
    "        execute_task(load_data, processed_data)\n",
    "    except Exception as e:\n",
    "        send_notification(f\"Pipeline failed with error: {e}\")\n",
    "\n",
    "# Run the flow\n",
    "etl_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic monitoring in a data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First, it makes synthetic data data and saves it to raw_data.csv.\n",
    "2. Next, it reads the data, changes the strings in raw_column to uppercase, squares numeric_feature, and rounds float_feature. It saves this to processed_data.csv.\n",
    "3. It has error handling to send notifications if something goes wrong.\n",
    "4. It also checks the data quality to make sure there are no missing values. The final CSV, processed_data.csv, has the original and transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define synthetic data creation task\n",
    "@task\n",
    "def create_synthetic_data():\n",
    "    np.random.seed(42)\n",
    "    data = {\n",
    "        'raw_column': np.random.choice(['apple', 'banana', 'cherry', 'date', 'elderberry'], 1000),\n",
    "        'numeric_feature': np.random.randint(1, 100, 1000),\n",
    "        'float_feature': np.random.uniform(0, 1, 1000)\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"raw_data.csv\", index=False)\n",
    "    print(f\"Created synthetic dataset with {len(df)} records\")\n",
    "    return df\n",
    "\n",
    "# Define data extraction task\n",
    "@task\n",
    "def extract_data():\n",
    "    # Simulating data extraction\n",
    "    df = pd.read_csv(\"raw_data.csv\")\n",
    "    print(f\"Extracted {len(df)} records\")\n",
    "    return df\n",
    "\n",
    "# Transformation function\n",
    "def some_transformation(x):\n",
    "    return x.upper()\n",
    "\n",
    "# Define data transformation task\n",
    "@task\n",
    "def transform_data(df):\n",
    "    # Performing data transformations\n",
    "    df['processed_column'] = df['raw_column'].apply(some_transformation)\n",
    "    df['numeric_squared'] = df['numeric_feature'] ** 2\n",
    "    df['float_rounded'] = df['float_feature'].round(2)\n",
    "    print(f\"Transformed {len(df)} records\")\n",
    "    return df\n",
    "\n",
    "# Define data loading task\n",
    "@task\n",
    "def load_data(df):\n",
    "    # Simulating data loading\n",
    "    df.to_csv(\"processed_data.csv\", index=False)\n",
    "    print(f\"Loaded {len(df)} records\")\n",
    "\n",
    "# Define data quality monitoring task\n",
    "@task\n",
    "def monitor_data_quality(df):\n",
    "    # Basic data quality checks\n",
    "    assert df['processed_column'].isnull().sum() == 0, \"Null values detected in processed_column\"\n",
    "    assert df['numeric_squared'].isnull().sum() == 0, \"Null values detected in numeric_squared\"\n",
    "    assert df['float_rounded'].isnull().sum() == 0, \"Null values detected in float_rounded\"\n",
    "    assert len(df) > 0, \"Empty dataframe detected\"\n",
    "    print(\"Data quality checks passed\")\n",
    "\n",
    "# Define the flow using Prefect 2.x\n",
    "@flow(name=\"Monitored ETL Pipeline\")\n",
    "def etl_pipeline():\n",
    "    # Create synthetic data\n",
    "    synthetic_data = create_synthetic_data()\n",
    "\n",
    "    # Extract data\n",
    "    raw_data = extract_data()\n",
    "\n",
    "    # Transform data\n",
    "    processed_data = transform_data(raw_data)\n",
    "\n",
    "    # Monitor data quality\n",
    "    monitor_data_quality(processed_data)\n",
    "\n",
    "    # Load data\n",
    "    load_data(processed_data)\n",
    "\n",
    "# Run the flow\n",
    "etl_pipeline()\n",
    "\n",
    "# Display the first few rows of the processed data\n",
    "processed_df = pd.read_csv(\"processed_data.csv\")\n",
    "print(\"\\nFirst few rows of processed data:\")\n",
    "print(processed_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
