{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Batch data ingestion process using Python**"
      ],
      "metadata": {
        "id": "rsVc5UjhWmUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The  Python scripts demonstrate an automated batch data ingestion process, utilizing synthetic transaction data to simulate daily activities.\n",
        "- The first script generates synthetic data, representing various transaction types over consecutive days, which are saved in a CSV file. This data includes randomized transaction amounts, categories, and dates, ensuring a diverse dataset for testing.\n",
        "- The second script automates the ingestion of this data into an SQLite database, scheduled to run daily.\n",
        "- This setup is particularly useful for testing and developing data processing workflows like ETL processes and database updates without the need for real, sensitive data."
      ],
      "metadata": {
        "id": "1BuY_yo7zX4N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLkikrjcWjip"
      },
      "outputs": [],
      "source": [
        "#Install required libraries\n",
        "!pip install schedule"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sqlalchemy import create_engine\n",
        "import schedule\n",
        "import time"
      ],
      "metadata": {
        "id": "f4JjpY2OWpFy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data. This data generator should not be edited as it simulates a fixed schema dataset.\n",
        "def generate_synthetic_data():\n",
        "    np.random.seed(0)\n",
        "    dates = pd.date_range(start='2021-01-01', periods=100, freq='D')\n",
        "    modes = ['Cash', 'Card', 'Online', 'Wallet']\n",
        "    categories = ['Grocery', 'Electronics', 'Apparel', 'Dining']\n",
        "    subcategories = ['Vegetables', 'Mobiles', 'Clothing', 'Restaurants']\n",
        "    notes = ['Purchase', 'Payment', 'Refund', 'Expense']\n",
        "    amounts = np.random.uniform(100, 5000, size=100)\n",
        "    income_expense = ['Income', 'Expense']\n",
        "    currency = 'INR'\n",
        "\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'Date': dates,\n",
        "        'Mode': np.random.choice(modes, size=100),\n",
        "        'Category': np.random.choice(categories, size=100),\n",
        "        'Subcategory': np.random.choice(subcategories, size=100),\n",
        "        'Note': np.random.choice(notes, size=100),\n",
        "        'Amount': amounts,\n",
        "        'Income/Expense': np.random.choice(income_expense, size=100),\n",
        "        'Currency': currency\n",
        "    })\n",
        "\n",
        "\n",
        "    df.to_csv('daily_transactions.csv', index=False)\n",
        "\n",
        "# Call the function to generate data\n",
        "generate_synthetic_data()"
      ],
      "metadata": {
        "id": "A_W8kPpNWqmT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Function to simulate batch data ingestion from a CSV file into an SQLite database.\n",
        "Note:\n",
        "- No need for database credentials or network setup.\n",
        "\"\"\"\n",
        "def batch_ingest():\n",
        "\n",
        "    df = pd.read_csv('daily_transactions.csv')\n",
        "    df['transaction_date'] = pd.to_datetime(df['Date'])\n",
        "    df['amount'] = df['Amount'].astype(float)\n",
        "\n",
        "    # Create an SQLite engine\n",
        "    engine = create_engine('sqlite:///mydatabase.db')\n",
        "\n",
        "    # Ingest data into the SQLite database\n",
        "    df.to_sql('transactions', engine, if_exists='append', index=False)\n",
        "\n",
        "    print(f\"Ingested {len(df)} records at {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Setting up a scheduler to run the batch ingestion daily at 1:00 AM\n",
        "schedule.every().day.at(\"01:00\").do(batch_ingest)\n",
        "\n",
        "# Infinite loop to keep the script running\n",
        "# Note: This will run indefinitely. User must manually stop execution.\n",
        "try:\n",
        "    while True:\n",
        "        schedule.run_pending()\n",
        "        time.sleep(5)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Batch ingestion stopped by user.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FKkeSslWuar",
        "outputId": "a540746c-ab37-42d6-e40f-641cf7e04b7e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch ingestion stopped by user.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Real-time data streaming using Python and Kafka**"
      ],
      "metadata": {
        "id": "amTEfg9eW3e0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The script demonstrates an implementation of real-time data streaming using Apache Kafka.\n",
        "- It creates a Kafka consumer to listen to the 'user_activity' topic, dynamically processing JSON messages as they arrive.\n",
        "- Each message is parsed to extract user activity details, which then triggers specific actions based on the type of activity."
      ],
      "metadata": {
        "id": "GPlBH6DyzQ0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install kafka-python"
      ],
      "metadata": {
        "id": "YB06do-3WuzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "from kafka import KafkaConsumer\n",
        "import json"
      ],
      "metadata": {
        "id": "rudWTmC8W5Z0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creates and returns a Kafka consumer listening on a specified topic with\n",
        "auto-commit enabled and latest offset reset.\n",
        "\"\"\"\n",
        "def create_kafka_consumer():\n",
        "    consumer = KafkaConsumer(\n",
        "        'user_activity',\n",
        "        bootstrap_servers=['localhost:9092'],\n",
        "        auto_offset_reset='latest',\n",
        "        enable_auto_commit=True,\n",
        "        value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
        "    )\n",
        "    return consumer\n",
        "\"\"\"\n",
        "Processes incoming Kafka messages. It performs operations based on the type of user activity,\n",
        "such as updating profiles or triggering security alerts.\n",
        "\"\"\"\n",
        "def process_messages(consumer):\n",
        "    for message in consumer:\n",
        "        user_activity = message.value\n",
        "        print(f\"Received user activity: {user_activity}\")\n",
        "        # Example: Update user profile or trigger alert based on activity type.\n",
        "        if user_activity['activity_type'] == 'purchase':\n",
        "            # This is a placeholder for your update function\n",
        "            update_user_profile(user_activity['user_id'], user_activity['item_id'])\n",
        "        elif user_activity['activity_type'] == 'login' and user_activity['location'] != 'usual_location':\n",
        "            # This is a placeholder for your security alert function\n",
        "            trigger_security_alert(user_activity['user_id'], user_activity['location'])\n",
        "\n",
        "\n",
        "def update_user_profile(user_id, item_id):\n",
        "    \"\"\"Placeholder function to update user profile\"\"\"\n",
        "    print(f\"Updating profile for user {user_id} with item {item_id}\")\n",
        "\n",
        "def trigger_security_alert(user_id, location):\n",
        "    \"\"\"Placeholder function to trigger security alert\"\"\"\n",
        "    print(f\"Security alert for user {user_id} logging in from {location}\")\n",
        "\n",
        "# Create the Kafka consumer\n",
        "consumer = create_kafka_consumer()\n",
        "\n",
        "# Process incoming messages in real-time\n",
        "process_messages(consumer)"
      ],
      "metadata": {
        "id": "TOwvgP59XB5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **API-based data collection using Python and the requests library**"
      ],
      "metadata": {
        "id": "lHYgnccPW_0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The script uses the requests library to access structured data via the OpenWeatherMap API.\n",
        "\n",
        "- It fetches weather data for a specified city and time range, converting the JSON response into a structured pandas DataFrame. This DataFrame is then saved as a CSV file."
      ],
      "metadata": {
        "id": "UAUN733oydZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install requests"
      ],
      "metadata": {
        "id": "x3euMhJcW9GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta"
      ],
      "metadata": {
        "id": "-0yidjz2XMzv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect weather forecast data using the OpenWeatherMap API.\n",
        "def collect_weather_data(api_key, city, days=7):\n",
        "\n",
        "    base_url = \"http://api.openweathermap.org/data/2.5/forecast\"\n",
        "    params = {\n",
        "        'q': city,\n",
        "        'appid': api_key,\n",
        "        'units': 'metric'\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        weather_data = []\n",
        "        for item in data['list']:\n",
        "            weather_data.append({\n",
        "                'datetime': item['dt_txt'],\n",
        "                'temperature': item['main']['temp'],\n",
        "                'humidity': item['main']['humidity'],\n",
        "                'description': item['weather'][0]['description']\n",
        "            })\n",
        "        return weather_data\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        return None\n",
        "# Convert the list of weather data to a DataFrame and save it as a CSV file.\n",
        "def process_weather_data(weather_data, city):\n",
        "    if weather_data:\n",
        "        weather_df = pd.DataFrame(weather_data)\n",
        "        print(weather_df.head())\n",
        "        weather_df.to_csv(f\"{city}_weather_forecast.csv\", index=False)\n",
        "    else:\n",
        "        print(\"No data to process.\")\n",
        "\n",
        "# Example usage of the function\n",
        "if __name__ == \"__main__\":\n",
        "    api_key = '3102dd06454b055d78f482f602e4c4a9'  # Example API key\n",
        "    city = 'Mexico'  # Example city\n",
        "    weather_data = collect_weather_data(api_key, city)\n",
        "\n",
        "    # Process the collected weather data\n",
        "    process_weather_data(weather_data, city)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C83LNjhXMp7",
        "outputId": "edf94c24-ebb1-42dd-b07a-4c3bf0976171"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              datetime  temperature  humidity      description\n",
            "0  2024-08-05 09:00:00        30.15        72    moderate rain\n",
            "1  2024-08-05 12:00:00        27.23        84    moderate rain\n",
            "2  2024-08-05 15:00:00        25.23        91    moderate rain\n",
            "3  2024-08-05 18:00:00        24.95        93       light rain\n",
            "4  2024-08-05 21:00:00        24.77        94  overcast clouds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Web scraping using Python and BeautifulSoup**"
      ],
      "metadata": {
        "id": "rI98frGnXS0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The script utilizes the BeautifulSoup library to perform web scraping, specifically designed to extract book information from a webpage.\n",
        "- It fetches data such as book titles, prices, and availability, which are typical HTML content elements identified by their tags and classes."
      ],
      "metadata": {
        "id": "i9YqCpQwzKqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install requests beautifulsoup4 pandas"
      ],
      "metadata": {
        "id": "JkQS6ljxXUMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "H-ivgN5cXY76"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape book information from a website using BeautifulSoup.\n",
        "def scrape_book_data(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    books = []\n",
        "    for book in soup.find_all('article', class_='product_pod'):\n",
        "        title = book.h3.a['title']\n",
        "        price = book.find('p', class_='price_color').text\n",
        "        availability = book.find('p', class_='instock availability').text.strip()\n",
        "\n",
        "        books.append({\n",
        "            'title': title,\n",
        "            'price': price,\n",
        "            'availability': availability\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(books)\n",
        "# Process the scraped book data and store the results in a CSV file.\n",
        "def process_book_data(books_df):\n",
        "    books_df.to_csv('data_science_books.csv', index=False)\n",
        "\n",
        "# Usage\n",
        "url = 'http://books.toscrape.com/catalogue/shoe-dog-a-memoir-by-the-creator-of-nike_831/index.html'\n",
        "books_df = scrape_book_data(url)\n",
        "\n",
        "print(books_df.head())\n",
        "process_book_data(books_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLlnxltQXZSj",
        "outputId": "d2059cf6-89f7-47f4-c3d5-ea2fd3ff8757"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title    price availability\n",
            "0  The 10% Entrepreneur: Live Your Startup Dream ...  Â£27.55     In stock\n",
            "1  The Third Wave: An Entrepreneurâs Vision of ...  Â£12.61     In stock\n",
            "2                             If I Run (If I Run #1)  Â£49.97     In stock\n",
            "3         Counted With the Stars (Out from Egypt #1)  Â£17.97     In stock\n",
            "4               Like Never Before (Walker Family #2)  Â£28.77     In stock\n"
          ]
        }
      ]
    }
  ]
}