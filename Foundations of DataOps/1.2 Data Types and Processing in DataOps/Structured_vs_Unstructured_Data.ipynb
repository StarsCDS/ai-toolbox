{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Example of how structured data might be represented and processed**"
      ],
      "metadata": {
        "id": "RLF4Z3mURS0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It creates a DataFrame with customer data\n",
        "- It separates the featuresand the target variable.\n",
        "- It splits the data into training and testing sets.\n",
        "- It trains a logistic regression model on the training data.\n",
        "- Finally, it evaluates the model's accuracy on the test data and prints the result"
      ],
      "metadata": {
        "id": "ECTjxSl10HO-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FQnv1cEuRLUc"
      },
      "outputs": [],
      "source": [
        "#Import important libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of structured data\n",
        "data = pd.DataFrame({\n",
        "    'customer_id': [1, 2, 3, 4, 5],\n",
        "    'age': [28, 35, 42, 50, 33],\n",
        "    'tenure': [12, 24, 36, 48, 6],\n",
        "    'monthly_charge': [50.0, 70.0, 100.0, 80.0, 65.0],\n",
        "    'churn': [0, 0, 1, 1, 0]\n",
        "})\n",
        "\n",
        "# Easy to perform operations on structured data\n",
        "X = data[['age', 'tenure', 'monthly_charge']]\n",
        "y = data['churn']\n",
        "\n",
        "# Simple to use in machine learning models\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "#Evaluate model\n",
        "print(\"Model accuracy:\", model.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnXKkvGHRZ4_",
        "outputId": "f92fe05e-f079-411c-f8d5-5415cda34135"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Processing unstructured text data**"
      ],
      "metadata": {
        "id": "TEhYjINYRfkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It downloads the necessary NLTK resources for tokenization and stopwords.\n",
        "\n",
        "- It defines a function to preprocess text by tokenizing it, converting to lowercase, removing stopwords, and keeping only alphabetic tokens.\n",
        "\n",
        "- It demonstrates this function on an example customer review, outputting the processed tokens."
      ],
      "metadata": {
        "id": "C4m1OcoF0M-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install nltk\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "DtL4hLgpRcrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "nfJxwZCYRjaW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stopwords and non-alphabetic tokens\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Example unstructured data\n",
        "customer_review = \"\"\"\n",
        "The product was great! It arrived on time and the quality exceeded my expectations.\n",
        "However, the customer service could use some improvement. Overall, Iâ€™m satisfied.\n",
        "\"\"\"\n",
        "\n",
        "# Preprocess the example text\n",
        "processed_tokens = preprocess_text(customer_review)\n",
        "print(\"Processed Tokens:\", processed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZKPw3GjRlVZ",
        "outputId": "d8032d3d-a6d3-43c0-f14a-d87f74be24eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Tokens: ['product', 'great', 'arrived', 'time', 'quality', 'exceeded', 'expectations', 'however', 'customer', 'service', 'could', 'use', 'improvement', 'overall', 'satisfied']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Working with semi-structured data in JSON format**"
      ],
      "metadata": {
        "id": "4jn9-NfLSBPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The code snippet loads JSON data, converts it to a pandas DataFrame, and then accesses specific nested data for analysis, demonstrating semi-structured data handling.\n",
        "\n",
        "- It utilizes pandas' json_normalize function to transform json strings into a structured DataFrame format, which is more suitable for analysis. This DataFrame is printed, displaying columns for each attribute, including nested specifications like processor details.\n",
        "\n",
        "- The script extracts and prints the processor type (\"Intel i7\") for the laptop by accessing nested JSON data. It achieves this by filtering the DataFrame for the row where the product name is \"Laptop\" and then selecting the processor information from the nested 'specs' column."
      ],
      "metadata": {
        "id": "Uf3InAeassD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the necessary libraries\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Example of semi-structured data in JSON format\n",
        "json_data = '''\n",
        "[\n",
        "  {\n",
        "    \"product_id\": \"P001\",\n",
        "    \"name\": \"Smartphone\",\n",
        "    \"price\": 599.99,\n",
        "    \"specs\": {\n",
        "      \"screen\": \"6.2 inch\",\n",
        "      \"battery\": \"4000 mAh\",\n",
        "      \"camera\": \"12 MP\"\n",
        "    }\n",
        "  },\n",
        "  {\n",
        "    \"product_id\": \"P002\",\n",
        "    \"name\": \"Laptop\",\n",
        "    \"price\": 1299.99,\n",
        "    \"specs\": {\n",
        "      \"processor\": \"Intel i7\",\n",
        "      \"ram\": \"16 GB\",\n",
        "      \"storage\": \"512 GB SSD\"\n",
        "    }\n",
        "  }\n",
        "]\n",
        "'''\n",
        "\n",
        "# Parse JSON data\n",
        "data = json.loads(json_data)\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df = pd.json_normalize(data)\n",
        "\n",
        "print(df)\n",
        "\n",
        "# Accessing nested data\n",
        "print(\"Processor of the laptop:\")\n",
        "print(df.loc[df['name'] == 'Laptop', 'specs.processor'].values[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrFEj53iRn26",
        "outputId": "0e950f05-c1a6-4f1a-ba81-35bcb546d7a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  product_id        name    price specs.screen specs.battery specs.camera  \\\n",
            "0       P001  Smartphone   599.99     6.2 inch      4000 mAh        12 MP   \n",
            "1       P002      Laptop  1299.99          NaN           NaN          NaN   \n",
            "\n",
            "  specs.processor specs.ram specs.storage  \n",
            "0             NaN       NaN           NaN  \n",
            "1        Intel i7     16 GB    512 GB SSD  \n",
            "Processor of the laptop:\n",
            "Intel i7\n"
          ]
        }
      ]
    }
  ]
}