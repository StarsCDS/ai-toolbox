{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqz0tOsN3lIs"
      },
      "source": [
        "# Data synchronization methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Nrdtz243lEz"
      },
      "source": [
        "This example demonstrates an incremental synchronization method that updates only the records that have changed since the last synchronization, using a timestamp column to track changeS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRMag6qD3gLb",
        "outputId": "f748200b-8295-464f-93cd-a5f0dcc787e6"
      },
      "outputs": [],
      "source": [
        "#Install the libraries\n",
        "!pip install pysqlite3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvactyjJ3gGb",
        "outputId": "a574de65-a2d8-4a07-e524-1e7175c36ee6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synchronization completed. 1 rows affected.\n",
            "Contents of the target database:\n",
            "(1, 'Initial data', '2020-01-01 10:00:00')\n",
            "(2, 'More data', '2020-01-02 12:00:00')\n"
          ]
        }
      ],
      "source": [
        "#Import the libraries\n",
        "import sqlite3\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Define the incremental synchronization function to synchronize data from a source database to a target database\n",
        "def incremental_sync(source_conn, target_conn, table_name, timestamp_column):\n",
        "    source_cur = source_conn.cursor()\n",
        "    target_cur = target_conn.cursor()\n",
        "\n",
        "    # Get the last synced timestamp\n",
        "    target_cur.execute(f\"SELECT MAX({timestamp_column}) FROM {table_name}\")\n",
        "    last_sync = target_cur.fetchone()[0] or datetime.min\n",
        "\n",
        "    # Fetch new or updated records from source\n",
        "    source_cur.execute(f'''\n",
        "    SELECT * FROM {table_name}\n",
        "    WHERE {timestamp_column} > ?\n",
        "    ORDER BY {timestamp_column}\n",
        "    ''', (last_sync,))\n",
        "\n",
        "    rows = source_cur.fetchall()\n",
        "\n",
        "    # Insert or update records in target\n",
        "    for row in rows:\n",
        "        target_cur.execute(f'''\n",
        "        INSERT INTO {table_name} (id, data, last_updated) VALUES (?, ?, ?)\n",
        "        ON CONFLICT(id) DO UPDATE SET\n",
        "        data=excluded.data, last_updated=excluded.last_updated\n",
        "        ''', row)\n",
        "\n",
        "    target_conn.commit()\n",
        "    print(f\"Synchronization completed. {target_cur.rowcount} rows affected.\")\n",
        "\n",
        "# Function to set up the environment and perform synchronization\n",
        "def setup_and_sync():\n",
        "    # Create new SQLite databases in memory\n",
        "    source_conn = sqlite3.connect(':memory:')\n",
        "    target_conn = sqlite3.connect(':memory:')\n",
        "\n",
        "    # Create tables in both source and target databases\n",
        "    source_conn.execute('''\n",
        "    CREATE TABLE customers (\n",
        "        id INTEGER PRIMARY KEY,\n",
        "        data TEXT,\n",
        "        last_updated TIMESTAMP\n",
        "    )''')\n",
        "    target_conn.execute('''\n",
        "    CREATE TABLE customers (\n",
        "        id INTEGER PRIMARY KEY,\n",
        "        data TEXT,\n",
        "        last_updated TIMESTAMP\n",
        "    )''')\n",
        "\n",
        "    # Insert initial data into source database\n",
        "    source_conn.execute(\"INSERT INTO customers (id, data, last_updated) VALUES (1, 'Initial data', '2020-01-01 10:00:00')\")\n",
        "    source_conn.execute(\"INSERT INTO customers (id, data, last_updated) VALUES (2, 'More data', '2020-01-02 12:00:00')\")\n",
        "    source_conn.commit()\n",
        "\n",
        "    # Perform synchronization\n",
        "    incremental_sync(source_conn, target_conn, 'customers', 'last_updated')\n",
        "\n",
        "    # Fetch and print the current state of the target database\n",
        "    target_rows = target_conn.execute(\"SELECT * FROM customers\")\n",
        "    print(\"Contents of the target database:\")\n",
        "    for row in target_rows:\n",
        "        print(row)\n",
        "\n",
        "    # Close connections\n",
        "    source_conn.close()\n",
        "    target_conn.close()\n",
        "\n",
        "# Call the setup and sync function\n",
        "setup_and_sync()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6DkcjunbqNi"
      },
      "source": [
        "# Incremental data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjVRK_qhiDkI"
      },
      "source": [
        "This code creates synthetic sales data, saves it to a CSV file, and incrementally loads new records into a SQLite database table based on the latest transaction date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J1HNpqk5dGCj",
        "outputId": "6197ff43-d170-4b16-dbcc-4135d74dc5e1"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install pandas sqlalchemy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQocEMVoZsuE",
        "outputId": "7d74c95a-c13d-4c50-8eb7-1de2c005a89f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 100 new records.\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine, text\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "def create_synthetic_data(file_name, num_records):\n",
        "    start_date = datetime.now() - timedelta(days=30)\n",
        "    data = {\n",
        "        'transaction_date': [start_date + timedelta(days=random.randint(0, 30)) for _ in range(num_records)],\n",
        "        'amount': [random.uniform(10, 1000) for _ in range(num_records)],\n",
        "        'customer_id': [random.randint(1, 100) for _ in range(num_records)]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(file_name, index=False)\n",
        "\n",
        "def incremental_load(source_file, target_table, engine, timestamp_col):\n",
        "    # Read the entire source file\n",
        "    df = pd.read_csv(source_file, parse_dates=[timestamp_col])\n",
        "\n",
        "    # Get the last loaded timestamp from the target table\n",
        "    last_loaded = pd.read_sql(f\"SELECT MAX({timestamp_col}) as last_timestamp FROM {target_table}\", engine).iloc[0]['last_timestamp']\n",
        "\n",
        "    if pd.isna(last_loaded):\n",
        "        last_loaded = datetime.min\n",
        "\n",
        "    # Filter for new or updated records\n",
        "    new_data = df[df[timestamp_col] > last_loaded]\n",
        "\n",
        "    if not new_data.empty:\n",
        "        # Load new data into the target table\n",
        "        new_data.to_sql(target_table, engine, if_exists='append', index=False)\n",
        "        print(f\"Loaded {len(new_data)} new records.\")\n",
        "    else:\n",
        "        print(\"No new data to load.\")\n",
        "\n",
        "# Create synthetic data\n",
        "create_synthetic_data('sales_data.csv', 100)\n",
        "\n",
        "# Example usage with SQLite\n",
        "engine = create_engine('sqlite:///datawarehouse.db')\n",
        "\n",
        "# Create table if it doesn't exist\n",
        "with engine.connect() as conn:\n",
        "    conn.execute(text('''\n",
        "    CREATE TABLE IF NOT EXISTS sales (\n",
        "        transaction_date TEXT,\n",
        "        amount REAL,\n",
        "        customer_id INTEGER\n",
        "    )\n",
        "    '''))\n",
        "\n",
        "incremental_load('sales_data.csv', 'sales', engine, 'transaction_date')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLnLTaM6dNlQ"
      },
      "source": [
        "# Error handling and recovery in data integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkTUEE7Zibb4"
      },
      "source": [
        "This code creates synthetic sales data, saves it to a CSV file, and attempts to load the data into a SQLite database table with retry logic and error handling. If the load fails, it retries up to three times with exponential backoff. Logging and custom exceptions are used to handle and log errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vH534UMfegAl",
        "outputId": "502056aa-42ef-453f-bcab-e6b99ac5a034"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install pandas sqlalchemy tenacity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ow5OE9qlf5gu",
        "outputId": "fdf6bb88-73cc-484c-e02b-1817550c9b6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synthetic data created in sales_data.csv\n",
            "Source data read successfully.\n",
            "Successfully loaded 100 rows into sales\n",
            "Integration process completed.\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "import logging\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class IntegrationError(Exception):\n",
        "    pass\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "def load_data(df, table_name, engine):\n",
        "    try:\n",
        "        with engine.begin() as connection:\n",
        "            df.to_sql(table_name, connection, if_exists='append', index=False)\n",
        "            logger.info(f\"Successfully loaded {len(df)} rows into {table_name}\")\n",
        "            print(f\"Successfully loaded {len(df)} rows into {table_name}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading data into {table_name}: {str(e)}\")\n",
        "        print(f\"Error loading data into {table_name}: {str(e)}\")\n",
        "        raise IntegrationError(f\"Failed to load data into {table_name}\")\n",
        "\n",
        "def integrate_data(source_file, target_table, db_url):\n",
        "    engine = create_engine(db_url)\n",
        "    try:\n",
        "        # Read source data\n",
        "        df = pd.read_csv(source_file)\n",
        "        print(\"Source data read successfully.\")\n",
        "\n",
        "        # Perform data validation\n",
        "        if df.empty:\n",
        "            raise IntegrationError(\"Source file is empty\")\n",
        "\n",
        "        # Attempt to load data with retry mechanism\n",
        "        load_data(df, target_table, engine)\n",
        "    except IntegrationError as e:\n",
        "        logger.error(f\"Integration error: {str(e)}\")\n",
        "        print(f\"Integration error: {str(e)}\")\n",
        "        # Implement recovery logic here (e.g., revert to last known good state)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error: {str(e)}\")\n",
        "        print(f\"Unexpected error: {str(e)}\")\n",
        "    finally:\n",
        "        engine.dispose()\n",
        "        print(\"Integration process completed.\")\n",
        "\n",
        "# Create synthetic data for testing\n",
        "def create_synthetic_data(file_name, num_records):\n",
        "    start_date = datetime.now() - timedelta(days=30)\n",
        "    data = {\n",
        "        'transaction_date': [start_date + timedelta(days=random.randint(0, 30)) for _ in range(num_records)],\n",
        "        'amount': [random.uniform(10, 1000) for _ in range(num_records)],\n",
        "        'customer_id': [random.randint(1, 100) for _ in range(num_records)]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(file_name, index=False)\n",
        "    print(f\"Synthetic data created in {file_name}\")\n",
        "\n",
        "# Create synthetic data\n",
        "create_synthetic_data('sales_data.csv', 100)\n",
        "\n",
        "# Example usage with SQLite\n",
        "integrate_data('sales_data.csv', 'sales', 'sqlite:///datawarehouse.db')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
